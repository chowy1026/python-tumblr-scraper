#!/usr/bin/python3

import sys
import os
import time
import argparse
# import urlparse
import urllib
import json
import requests

# from tumblr_scraper import __version__

def uprint(msg, newline=True):
    """
    Unbuffered print.
    """
    if not quiet:
        sys.stdout.write("%s%s" % (msg, "\n" if newline else ''))
        sys.stdout.flush()

def get_blog_url(name, post_type=None):
    return "https://api.tumblr.com/v2/blog/{name}.tumblr.com/posts/{type}".format(name=name, type=post_type) if post_type else "https://api.tumblr.com/v2/blog/{name}.tumblr.com/posts".format(name=name)

    # "http://{name}.tumblr.com/api/read/json".format(name=name)

def get_json_page(url, offset=0, limit=20):
    args = {}
    args['offset'] = offset
    args['limit'] = limit
    args['api_key'] = "fuiKNFp9vQFvjLNvx4sUwti4Yb5yGutBN4Xh10LXZhhRKjWlV4"

    # Fetch the page
    r = requests.get(url, params=args)
    # print(r.url)

    # Strip the jsonp garbage
    text = r.text.strip()[45:-1]

    # Return the result as a dict
    return json.loads(text)


def create_html_with_string(title, summary, sdate, body, out_dir, slug, sformat):
    # Output filename
    filename = slug + "." + sformat
    html_out = os.path.join(out_dir, filename)

    # Doc Declaration
    str_doc = "<!DOCTYPE html>"

    # Head Open
    str_html_head_open = "<html lang='en'><head><meta charset='utf-8'><meta http-equiv='X-UA-Compatible' content='IE=edge'><meta name='viewport' content='width=device-width, initial-scale=1'><!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->"

    # Title in Head tag
    str_title = "<title>" + title + "</title>"

    # Head Close
    str_head_close = "<!-- Bootstrap --><link href='css/bootstrap.min.css' rel='stylesheet'><!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries --><!-- WARNING: Respond.js doesn't work if you view the page via file:// --><!--[if lt IE 9]><script src='https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js'></script><script src='https://oss.maxcdn.com/respond/1.4.2/respond.min.js'></script><![endif]--></head>"

    # Body Content
    str_body_open = "<body><h1>" + summary + "</h1><strong>Posted on: </strong>" + sdate + "<div>" + body + "<div>"

    # Body & HTML Close
    str_body_html_close = "<!-- jQuery (necessary for Bootstrap's JavaScript plugins) --><script src='https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js'></script><!-- Include all compiled plugins (below), or include individual files as needed --><script src='js/bootstrap.min.js'></script></body></html>"

    # Ensure the out directory exists
    try:
        os.makedirs(out_dir)
    except OSError:
        pass

    strHTML = str_doc + str_html_head_open + str_title + str_head_close + str_body_open + str_body_html_close
    with open(html_out, 'w') as f:
        f.write(strHTML)


def create_html_with_template(title, summary, sdate, body, out_dir, html_out):


    page_template = """
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
        <title>{title}</title>

        <!-- Bootstrap -->
        <link href="css/bootstrap.min.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->
      </head>
      <body>
        <h1>{summary}</h1>
        <strong>Posted on: </strong>{sdate}
        <div>
            {body}
        </div>
        <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
        <!-- Include all compiled plugins (below), or include individual files as needed -->
        <script src="js/bootstrap.min.js"></script>
      </body>
    </html>
    """

    # Ensure the out directory exists
    try:
        os.makedirs(out_dir)
    except OSError:
        pass

    page_template = page_template.format(title=title, summary=summary, sdate=sdate, body=body)
    with open(html_out, 'w') as f:
        f.write(page_template)


def save_photo(s, url, out_dir):
    # Get the URL path
    # url_path = urlparse.urlsplit(url).path
    url_path = urllib.parse.urlparse(url).path

    # Get the filename from the URL path
    filename = os.path.basename(url_path)

    # Set our local out file
    photo_out = os.path.join(out_dir, filename)

    # Ensure the out directory exists
    try:
        os.makedirs(out_dir)
    except OSError:
        pass

    # Bail if this file already exists
    if os.path.isfile(photo_out):
        return
    else:
        resp = s.get(url, stream=True)
        if resp.ok:
            with open(photo_out, 'wb') as fd:
                for chunk in resp.iter_content(chunk_size=1024):
                    fd.write(chunk)

def save_photo_caption(summary, sdate, caption, out_dir, slug, sformat):
    # Output filename
    filename = slug + "_photo." + sformat
    html_out = os.path.join(out_dir, filename)

    # INFO
    uprint('    ' + html_out)
    create_html_with_template(summary, summary, sdate, caption, out_dir, html_out)

def save_text_post(title, summary, sdate, body, out_dir, slug, sformat):
    # Output filename
    filename = slug + "_text." + sformat
    html_out = os.path.join(out_dir, filename)

    # INFO
    uprint('    ' + html_out)
    create_html_with_template(title, summary, sdate, body, out_dir, html_out)

def save_link_post(title, summary, sdate, body, out_dir, slug, sformat):
    # Output filename
    filename = slug + "_link." + sformat
    html_out = os.path.join(out_dir, filename)

    # INFO
    uprint('    ' + html_out)
    create_html_with_template(title, summary, sdate, body, out_dir, html_out)

def create_link_post_body():
    pass
    return

def save_post(json):
    # Get the GMT date components for this post
    year = json['date'][:4]
    month = json['date'][5:7]
    day = json['date'][8:10]

    # Find the out directory for this post
    out_dir = os.path.join(out, year, month, day, json['slug'])

    # INFO
    uprint(out_dir)

    # Create our HTTP session
    s = requests.Session()

    # Grab all the photos
    if json['type'] == 'photo':
        # Save photo captions to HTML
        save_photo_caption(json['summary'], str(json['date']), json['caption'], out_dir, json['slug'], json['format'])

        # Loop through to find and download photo in highest res
        for photo in json['photos']:
            photo_url = photo['alt_sizes'][0]['url']

            # INFO
            uprint('    ' + photo_url)
            # Download photo
            save_photo(s, photo_url, out_dir)

    elif json['type'] == 'text':

        save_text_post(json['title'], json['summary'], str(json['date']), json['body'], out_dir, json['slug'], json['format'])


    # elif json['type'] == 'link':
    #
    #     save_link_post(json['summary'], str(json['date']), json['caption'], out_dir, json['slug'], json['format'])
    #
    #     # Loop through to find and download photo in highest res
    #     for photo in json['photos']:
    #         photo_url = photo['alt_sizes'][0]['url']
    #
    #         # INFO
    #         uprint('    ' + photo_url)
    #         # Download photo
    #         save_photo(s, photo_url, out_dir)

def main():
    # Create our parser
    global parser
    parser = argparse.ArgumentParser(prog='tumblr-scraper',
            description='Archives a Tumblr blog to a local directory')

    # Set up our command-line arguments
    parser.add_argument('blog_name')
    parser.add_argument('-ptype', '--post_type', default=None, help='select post type to archive')

    parser.add_argument('-o', '--out', default=os.getcwd(),
            help='set the out directory for the archive')
    parser.add_argument('--quiet', action='store_true',
            help='suppress all output except errors')
    # parser.add_argument('--version', action='version',
    #         version='%(prog)s {v}'.format(v=__version__))

    # Get our arguments
    args = parser.parse_args()

    # Set our globals
    global quiet
    quiet = args.quiet

    global out
    out = os.path.expanduser(args.out)

    if not os.path.exists(out):
        parser.error('The out dir does not exist!')
    if not os.path.isdir(out):
        parser.error('The out dir is not a directory!')
    if not os.access(out, os.W_OK):
        parser.error('The out dir is not writable!')
    if not os.access(out, os.X_OK):
        parser.error('The out dir is not executable!')

    # Get our args
    blog_name = args.blog_name
    post_type = args.post_type
    # blog_url = get_blog_url(blog_name)
    blog_url = get_blog_url(blog_name, post_type)
    # print("blog_url: ", blog_url)


    # INFO
    uprint('Archiving \'%s\' to %s' % (blog_name, out,))

    # Get the total post count
    # Set limit to higher than to total blog posts.
    json = get_json_page(blog_url, 0, 1000)

    # total_count = json['posts-total']
    total_count = json['total_posts']

    # INFO
    uprint('Total posts: %s' % (total_count,))

    # Start our elapsed timer
    start_time = time.time()

    # This is the main loop. We'll loop over each page of post
    # results until we've archived the entire blog.
    offset = 0
    limit = 20
    while offset < total_count:
        json = get_json_page(blog_url, offset, limit)

        # Loop over each post in this batch
        for post in json['posts']:
            save_post(post)

        # Increment and grab the next batch of posts
        offset += limit

    # INFO
    minutes, seconds = divmod(time.time() - start_time, 60)
    uprint('Archived {count} posts in {m:02d}m and {s:02d}s'.format(\
            count=total_count, m=int(round(minutes)), s=int(round(seconds))))

if __name__ == '__main__':
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        sys.exit()
