#!/usr/bin/python3

import sys
import os
import time
import argparse
# import urlparse
import urllib
import json
import requests

# from tumblr_scraper import __version__

def uprint(msg, newline=True):
    """
    Unbuffered print.
    """
    if not quiet:
        sys.stdout.write("%s%s" % (msg, "\n" if newline else ''))
        sys.stdout.flush()

def get_blog_url(name, post_type=None):
    return "https://api.tumblr.com/v2/blog/{name}.tumblr.com/posts/{type}".format(name=name, type=post_type) if post_type else "https://api.tumblr.com/v2/blog/{name}.tumblr.com/posts".format(name=name)

    # "http://{name}.tumblr.com/api/read/json".format(name=name)

def get_json_page(url, api_key, offset=0, limit=20):
    args = {}
    args['offset'] = offset
    args['limit'] = limit
    args['api_key'] = api_key

    # Fetch the page
    r = requests.get(url, params=args)
    uprint('Request url with params :  %s ' % (r.url,))

    # Strip the jsonp garbage
    text = r.text.strip()[45:-1]

    # Return the result as a dict
    return json.loads(text)


def create_html_with_string(title, summary, sdate, body, out_dir, slug, sformat):
    # Output filename
    filename = slug + "." + sformat
    html_out = os.path.join(out_dir, filename)

    # Doc Declaration
    str_doc = "<!DOCTYPE html>"

    # Head Open
    str_html_head_open = "<html lang='en'><head><meta charset='utf-8'><meta http-equiv='X-UA-Compatible' content='IE=edge'><meta name='viewport' content='width=device-width, initial-scale=1'><!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->"

    # Title in Head tag
    str_title = "<title>" + title + "</title>"

    # Head Close
    str_head_close = "<!-- Bootstrap --><link href='../../../../css/bootstrap.min.css' rel='stylesheet'><!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries --><!-- WARNING: Respond.js doesn't work if you view the page via file:// --><!--[if lt IE 9]><script src='https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js'></script><script src='https://oss.maxcdn.com/respond/1.4.2/respond.min.js'></script><![endif]--></head>"

    # Body Content
    str_body_open = "<body><h1>" + summary + "</h1><strong>Posted on: </strong>" + sdate + "<div>" + body + "<div>"

    # Body & HTML Close
    str_body_html_close = "<!-- jQuery (necessary for Bootstrap's JavaScript plugins) --><script src='https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js'></script><!-- Include all compiled plugins (below), or include individual files as needed --><script src='../../../../js/bootstrap.min.js'></script></body></html>"

    # Ensure the out directory exists
    try:
        os.makedirs(out_dir)
    except OSError:
        pass

    strHTML = str_doc + str_html_head_open + str_title + str_head_close + str_body_open + str_body_html_close

    if os.path.isfile(html_out):
        return
    else:
        with open(html_out, 'w') as f:
            f.write(strHTML)


def create_html_with_template(title, summary, sdate, body, out_dir, html_out):


    page_template = """
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
        <title>{title}</title>

        <!-- Bootstrap -->
        <link href="../../../../css/bootstrap.min.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->
      </head>
      <body>
        <h1>{summary}</h1>
        <strong>Posted on: </strong>{sdate}
        <div>
            {body}
        </div>
        <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
        <!-- Include all compiled plugins (below), or include individual files as needed -->
        <script src="../../../../js/bootstrap.min.js"></script>
      </body>
    </html>
    """

    # Ensure the out directory exists
    try:
        os.makedirs(out_dir)
    except OSError:
        pass

    page_template = page_template.format(title=title, summary=summary, sdate=sdate, body=body)
    if os.path.isfile(html_out):
        return
    else:
        with open(html_out, 'w') as f:
            f.write(page_template)

def get_local_photo_path(url, out_dir):

    # url_path = urlparse.urlsplit(url).path
    url_path = urllib.parse.urlparse(url).path

    # Get the filename from the URL path
    filename = os.path.basename(url_path)

    # Set our local out file
    photo_out = os.path.join(out_dir, filename)
    return photo_out

def save_photo(s, url, out_dir):

    # Get the URL path
    photo_out = get_local_photo_path(url, out_dir)

    # Ensure the out directory exists
    try:
        os.makedirs(out_dir)
    except OSError:
        pass

    # Bail if this file already exists
    if os.path.isfile(photo_out):
        return
    else:
        resp = s.get(url, stream=True)
        if resp.ok:
            with open(photo_out, 'wb') as fd:
                for chunk in resp.iter_content(chunk_size=1024):
                    fd.write(chunk)


def save_photo_caption(summary, sdate, caption, photo_url, out_dir, slug, sformat):
    # Output filename
    filename = slug + "_photo." + sformat
    html_out = os.path.join(out_dir, filename)

    # Get filename from path, same directory with HTML
    relative_photo_path = os.path.basename(photo_url)

    # body template for photo posts
    body_template = """
    <img src = "{photo_out}"
         alt = "{summary}" /><br/>
    <h4>{summary}</h4>
    <p>{caption}</p>
    """
    body_template = body_template.format(photo_out=relative_photo_path, summary=summary, caption=caption)

    create_html_with_template(summary, summary, sdate, body_template, out_dir, html_out)


def save_text_post(title, summary, sdate, body, out_dir, slug, sformat):
    # Output filename
    filename = slug + "_text." + sformat
    html_out = os.path.join(out_dir, filename)


    create_html_with_template(title, summary, sdate, body, out_dir, html_out)


def save_link_post(title, summary, sdate, url, photo_url, link_author, excerpt, publisher, description, out_dir, slug, sformat):
    # Output filename
    filename = slug + "_link." + sformat
    html_out = os.path.join(out_dir, filename)

    # Get filename from path, same directory with HTML
    relative_photo_path = os.path.basename(photo_url) if photo_url else None

    # link template for link posts
    img_template = """
        <img src = "{relative_photo_path}" alt = "{title}" /><br/>
    """.format(relative_photo_path=relative_photo_path, title=title)

    # link template for link posts
    link_template = """
    <a href="{url}" target="_blank"><h4>{summary}</h4></a><br/>
    <p>"{excerpt}..." by {link_author} @ {publisher}</p>
    <p>{description}</p>
    """.format(url=url, summary=summary, excerpt=excerpt, link_author=link_author, publisher=publisher, description=description)

    body_template = img_template + link_template if photo_url else link_template

    create_html_with_template(title, summary, sdate, body_template, out_dir, html_out)


def save_post(json):
    # Get the GMT date components for this post
    year = json['date'][:4]
    month = json['date'][5:7]
    day = json['date'][8:10]

    # Find the out directory for this post
    out_dir = os.path.join(out, json['blog_name'], year, month, day, json['slug'])

    # Create our HTTP session
    s = requests.Session()

    # Grab all the photos
    if json['type'] == 'photo':

        # Loop through to find and download photo in highest res
        for photo in json['photos']:
            photo_url = photo['original_size']['url']

            # Download photo
            save_photo(s, photo_url, out_dir)
            # Save photo captions to HTML
            save_photo_caption(json['summary'], str(json['date']), json['caption'], photo_url, out_dir, json['slug'], json['format'])

    elif json['type'] == 'text':

        save_text_post(json['title'], json['summary'], str(json['date']), json['body'], out_dir, json['slug'], json['format'])

    elif json['type'] == 'link':
        photo_url = None
        if json.get('link_image'):

            for photo in json['photos']:
                photo_url = photo['original_size']['url']

                # Download photo
                save_photo(s, photo_url, out_dir)
        else:
            photo_url = None

        # link post to HTML
        save_link_post(json['title'], json['summary'], str(json['date']), json['url'], photo_url, json['link_author'], json['excerpt'], json['publisher'], json['description'], out_dir, json['slug'], json['format'])



def main():
    # Create our parser
    global parser
    parser = argparse.ArgumentParser(prog='tumblr-scraper',
            description='Archives a Tumblr blog to a local directory')

    # Set up our command-line arguments
    parser.add_argument('blog_name')
    parser.add_argument('api_key')
    parser.add_argument('-ptype', '--post_type', default=None, help='select post type to archive')


    parser.add_argument('-o', '--out', default=os.getcwd(),
            help='set the out directory for the archive')
    parser.add_argument('--quiet', action='store_true',
            help='suppress all output except errors')
    # parser.add_argument('--version', action='version',
    #         version='%(prog)s {v}'.format(v=__version__))

    # Get our arguments
    args = parser.parse_args()

    # Set our globals
    global quiet
    quiet = args.quiet

    global out
    out = os.path.expanduser(args.out)

    if not os.path.exists(out):
        parser.error('The out dir does not exist!')
    if not os.path.isdir(out):
        parser.error('The out dir is not a directory!')
    if not os.access(out, os.W_OK):
        parser.error('The out dir is not writable!')
    if not os.access(out, os.X_OK):
        parser.error('The out dir is not executable!')

    # Get our args
    blog_name = args.blog_name
    api_key = args.api_key
    post_type = args.post_type

    blog_url = get_blog_url(blog_name, post_type)



    # INFO
    uprint('Archiving \'%s\' to %s' % (blog_name, out,))

    # Get the total post count
    # Set limit to higher than to total blog posts.
    json = get_json_page(blog_url, api_key, 0, 1000)

    # total_count = json['posts-total']
    total_count = json['total_posts']

    # INFO
    uprint('Total posts: %s' % (total_count,))

    # Start our elapsed timer
    start_time = time.time()

    # This is the main loop. We'll loop over each page of post
    # results until we've archived the entire blog.
    offset = 0
    limit = 20
    saved_count = 0
    while offset < total_count:
        json = get_json_page(blog_url, api_key, offset, limit)

        # Loop over each post in this batch
        for post in json['posts']:
            save_post(post)
            saved_count += 1
            # INFO
            uprint('%s post saved.' % (saved_count,))

        # Increment and grab the next batch of posts
        offset += limit

    # INFO
    minutes, seconds = divmod(time.time() - start_time, 60)
    uprint('Archived {count} posts in {m:02d}m and {s:02d}s'.format(\
            count=total_count, m=int(round(minutes)), s=int(round(seconds))))

if __name__ == '__main__':
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        sys.exit()
